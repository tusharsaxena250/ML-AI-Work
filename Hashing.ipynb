{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from random import sample\n",
    "import shutil\n",
    "from zipfile import ZipFile ## Download this library if not available\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") ##Ignore all sort of warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting data from zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dir = os.getcwd() ## my_dir is my current working folder\n",
    "zip_folder = os.path.join(my_dir,\"yalefaces.zip\")\n",
    "print(\"Path to the zipped folder is {}\".format(zip_folder))\n",
    "with ZipFile(zip_folder, 'r') as zip: \n",
    "    zip.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with data directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(my_dir,\"yaleface\")\n",
    "file_list = os.listdir(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder_path = os.path.join(my_dir,\"Faces_Train\")\n",
    "test_folder_path = os.path.join(my_dir,\"Faces_Test\")\n",
    "\n",
    "## Delete the folders if they exist already\n",
    "if os.path.exists(train_folder_path):\n",
    "    shutil.rmtree(train_folder_path)\n",
    "\n",
    "if os.path.exists(test_folder_path):\n",
    "    shutil.rmtree(test_folder_path)\n",
    "\n",
    "os.mkdir(train_folder_path)  ## Creates a new directory\n",
    "os.mkdir(test_folder_path) ## Creates a new directory\n",
    "\n",
    "idx_list = [str(i).zfill(2) for i in range(1,16,1)] ##Creates numbers with leading zeros(i.e. 01 instead of 1, 02 instaed of 2 etc.)\n",
    "print(idx_list)\n",
    "\n",
    "file_name_list = [[] for i in range(15)]\n",
    "\n",
    "for i in range(len(idx_list)):\n",
    "    for fname in file_list:\n",
    "        if fname.startswith(\"subject\"+idx_list[i]):\n",
    "            file_name_list[i].append(os.path.join(data_folder,fname))\n",
    "\n",
    "print(file_name_list)\n",
    "\n",
    "for i in range(len(idx_list)):\n",
    "    ls = file_name_list[i]  ## Finds a list within the 'file_name_list',which contains the paths to images of a particular subject\n",
    "    \n",
    "    ## Since a single list contains paths to the images of a particular subject and each subject has 11 images, we will randomly choose an index between 0 and 11\n",
    "    test_idx = np.random.choice(11)  \n",
    "    test_file = ls[test_idx]\n",
    "    shutil.copy(test_file,test_folder_path)\n",
    "    \n",
    "    ls.remove(ls[test_idx])\n",
    "    \n",
    "    for train_file in ls:\n",
    "        shutil.copy(train_file,train_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files_list = os.listdir(train_folder_path)\n",
    "#print(train_files_list)\n",
    "train_ls = []\n",
    "for file in train_files_list:\n",
    "    img_file = os.path.join(train_folder_path,file)\n",
    "    arr = mpimg.imread(img_file)\n",
    "    #print(arr.shape)\n",
    "    arr = arr.reshape(1,arr.shape[0]*arr.shape[1]) ## Convert to a 1D matrix\n",
    "    train_ls.append(np.ravel(arr)) ## Before appending, convert the 1D martix to a 1d array using np.ravel \n",
    "train_mat = np.matrix(train_ls)\n",
    "print(train_mat.shape)\n",
    "mean_img = np.mean(train_mat,axis=0)\n",
    "print(mean_img.shape)\n",
    "cov = np.cov(train_mat)\n",
    "print(cov.shape)\n",
    "eig_val,eig_vec = np.linalg.eig(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_vec_ls = []\n",
    "#eig1 = train_mat.T@eig_vec[:,0]\n",
    "#print(eig1.shape)\n",
    "for i in range(eig_vec.shape[1]):\n",
    "    eig1 = train_mat.T@eig_vec[:,i]\n",
    "    eig1 = eig1/eig_val[i]\n",
    "    eigen_vec_ls.append(np.ravel(eig1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = np.argsort(eig_val) ## indices for eigenvalues which are in ascending order\n",
    "sort_idx = sort_idx[::-1]\n",
    "\n",
    "eig_val_sum = np.sum(eig_val)\n",
    "temp_sum = 0\n",
    "principal_eig_vec = []\n",
    "principal_eig_val = []\n",
    "i=0\n",
    "while(temp_sum<0.95*eig_val_sum):\n",
    "    principal_eig_vec.append(eigen_vec_ls[sort_idx[i]])\n",
    "    principal_eig_val.append(eig_val[sort_idx[i]])\n",
    "    temp_sum += eig_val[sort_idx[i]]\n",
    "    i += 1\n",
    "print(\"Number of components is {}\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = mean_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_hat = np.matrix(principal_eig_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mu)\n",
    "print(Q_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOL 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = os.listdir(test_folder_path)\n",
    "feat_vec_ls = []\n",
    "for file in test_files:\n",
    "    img_file = os.path.join(test_folder_path,file)\n",
    "    test_img = mpimg.imread(img_file)\n",
    "    test_img = test_img.reshape(arr.shape[0]*arr.shape[1],1)\n",
    "    test_img = test_img - mean_img.T\n",
    "    #print(np.linalg.pinv(Q_hat).shape,test_img.shape)\n",
    "    feat_vec = np.linalg.pinv(Q_hat).T@test_img\n",
    "    feat_vec_ls.append(np.ravel(feat_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feat_vec_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOL 4 50 random vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_len = len(feat_vec_ls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genRandomHashVector(m, length):\n",
    "    hash_vector = []\n",
    "    for i in range(m):\n",
    "        v = np.random.uniform(-1, 1, length)\n",
    "        vcap = v / np.linalg.norm(v)\n",
    "        hash_vector.append(vcap)\n",
    "    return hash_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_vec = genRandomHashVector(50, vec_len)\n",
    "print(hash_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOL 5 50bit Hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localSensitiveHashing(hash_vector, data):\n",
    "    hash_code = []\n",
    "    for i in range(len(hash_vector)):\n",
    "        if np.dot(data, hash_vector[i]) > 0:\n",
    "            hash_code.append('1')\n",
    "        else:\n",
    "            hash_code.append('0')\n",
    "    return hash_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0', '0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '1', '0', '1', '0', '0', '0', '1', '1', '0', '0', '0', '1', '1', '1', '0', '1', '1', '1', '0', '1', '0', '1', '0', '1', '0', '0', '1', '0', '1', '0', '0', '1', '1', '1', '1', '0', '1', '1', '0'], ['0', '0', '1', '0', '0', '1', '1', '0', '0', '1', '1', '1', '0', '0', '0', '0', '0', '1', '1', '1', '0', '0', '1', '1', '1', '1', '0', '0', '0', '1', '1', '0', '1', '0', '1', '0', '1', '1', '0', '1', '0', '0', '0', '1', '1', '1', '1', '1', '1', '0'], ['0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '1', '0', '1', '0', '0', '1', '1', '1', '0', '0', '1', '1', '0', '0', '0', '1', '0', '0', '0', '1', '1', '1', '0', '1', '1', '1', '1', '0', '0', '0', '1', '1', '1', '0', '0', '0', '0', '1', '0', '1'], ['0', '0', '1', '1', '0', '1', '1', '0', '0', '1', '0', '0', '1', '0', '0', '1', '0', '1', '1', '0', '1', '1', '1', '0', '0', '1', '0', '1', '0', '1', '1', '1', '1', '0', '1', '0', '1', '0', '0', '1', '0', '1', '0', '1', '1', '0', '1', '1', '1', '1'], ['0', '0', '1', '0', '0', '1', '1', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '1', '1', '1', '0', '0', '1', '1', '1', '1', '0', '0', '0', '1', '1', '0', '1', '0', '1', '0', '1', '1', '0', '1', '0', '0', '0', '1', '1', '1', '1', '1', '1', '0'], ['1', '1', '0', '0', '1', '0', '0', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '0', '0', '1', '0', '0', '1', '1', '1', '0', '1', '1', '1', '0', '0', '1', '1', '1', '0', '1', '0', '0', '1', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '1'], ['0', '0', '1', '1', '0', '1', '0', '0', '1', '0', '1', '0', '1', '0', '0', '1', '1', '1', '0', '0', '1', '1', '0', '0', '0', '1', '0', '0', '0', '1', '1', '1', '0', '1', '1', '1', '1', '0', '0', '0', '1', '1', '0', '1', '0', '0', '1', '1', '0', '1'], ['1', '0', '0', '1', '0', '1', '0', '1', '1', '1', '1', '0', '1', '0', '0', '1', '1', '1', '0', '0', '1', '1', '0', '0', '0', '1', '0', '0', '1', '0', '1', '1', '0', '1', '1', '1', '0', '0', '1', '0', '1', '1', '0', '1', '0', '0', '0', '1', '0', '1'], ['1', '1', '0', '0', '1', '0', '0', '1', '1', '1', '1', '0', '1', '1', '1', '1', '1', '0', '0', '0', '1', '1', '0', '1', '0', '0', '1', '1', '1', '0', '0', '1', '0', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '0', '0', '0', '0', '0', '0', '1'], ['0', '0', '1', '1', '0', '1', '1', '0', '1', '1', '1', '0', '1', '0', '0', '1', '1', '1', '0', '0', '1', '1', '0', '0', '0', '1', '0', '0', '0', '1', '1', '1', '0', '0', '1', '1', '1', '0', '0', '1', '1', '1', '0', '1', '0', '0', '1', '1', '0', '1'], ['0', '0', '1', '1', '0', '1', '1', '0', '0', '0', '1', '0', '1', '0', '0', '0', '0', '1', '1', '0', '1', '1', '1', '0', '1', '1', '0', '0', '0', '1', '1', '0', '0', '1', '1', '1', '1', '0', '0', '1', '1', '1', '1', '1', '0', '0', '1', '1', '1', '0'], ['0', '0', '1', '0', '1', '1', '1', '1', '0', '0', '1', '1', '0', '1', '1', '0', '0', '1', '1', '1', '0', '0', '1', '1', '1', '1', '0', '0', '0', '1', '1', '0', '1', '0', '1', '0', '1', '1', '0', '1', '0', '0', '0', '1', '1', '1', '1', '1', '1', '0'], ['0', '0', '1', '1', '0', '1', '1', '0', '0', '0', '0', '0', '1', '0', '0', '1', '1', '1', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0', '0', '1', '1', '1', '0', '0', '1', '0', '0', '1', '0', '1', '1', '1', '0', '1', '1', '0', '1', '1', '0', '1'], ['1', '1', '0', '0', '1', '0', '0', '1', '1', '1', '0', '0', '0', '1', '1', '1', '1', '0', '0', '1', '0', '1', '0', '1', '1', '0', '1', '1', '1', '0', '0', '1', '0', '1', '0', '1', '0', '0', '1', '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '1'], ['0', '0', '0', '1', '0', '1', '0', '1', '0', '1', '1', '0', '1', '0', '0', '1', '1', '1', '1', '0', '1', '1', '0', '0', '0', '1', '0', '0', '1', '0', '1', '1', '0', '1', '1', '1', '1', '0', '0', '1', '1', '1', '0', '1', '1', '0', '0', '1', '0', '1']]\n"
     ]
    }
   ],
   "source": [
    "hashes = []\n",
    "for i in range(len(feat_vec_ls)):\n",
    "    hash_cd = localSensitiveHashing(hash_vec, feat_vec_ls[i])\n",
    "    hashes.append(hash_cd)\n",
    "print(hashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOL 6 L1 NORM betweed hash reps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.random.randint(len(feat_vec_ls))\n",
    "list_1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(feat_vec_ls)):\n",
    "    list_1.append(np.linalg.norm((feat_vec_ls[i] - feat_vec_ls[n]), ord=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 27158.277765675004, 32608.51719382895, 37606.30442819892, 38804.363306266496, 39861.33568237125, 40626.107263963575, 42999.22701332929, 48059.57508748153, 48259.02211363749, 52964.524740514986, 53768.99244572675, 61104.50600520018, 74539.76155548061, 99395.93471657355]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(list_1)) #sorted list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
